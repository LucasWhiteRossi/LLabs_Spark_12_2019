{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio: Engenharia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "Leia o arquivo de texto ​ wordcount.txt ​ e conte as palavras que contém até 10 letras. Conte\n",
    "também quantas palavras com mais de 10 letras existem no texto.\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt\n",
    "\n",
    "Exemplo do dataset final:\n",
    "[('two', 2),\n",
    "('behold', 1),\n",
    "('itself', 3),\n",
    "(‘MAIORES QUE 10’, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte a ser adaptada com a referência do pacote pyspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/lucas/spark-3.0.0-preview-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca Task1\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               IntegerType, \n",
    "                               ArrayType, \n",
    "                               StringType,\n",
    "                               StructField)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sessão\n",
    "spark = SparkSession.builder.appName('desafio').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(value,StringType,true)))\n",
      "\n",
      "\n",
      "['value']\n"
     ]
    }
   ],
   "source": [
    "# Leitura do texto\n",
    "\n",
    "url1 = 'https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt'\n",
    "\n",
    "spark.sparkContext.addFile(url1)\n",
    "\n",
    "text_task_1 = spark.read.text(\"file://\" + SparkFiles.get('wordcount.txt'), wholetext=True, lineSep=None)\n",
    "\n",
    "print(text_task_1.schema)\n",
    "#print(text_task_1.head())\n",
    "print('\\n')\n",
    "print(text_task_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretendo selecionar todas as palavras eliminando todo o tipo de pontuação, espaço ou símbolo especial.\n",
    "\n",
    "# Para isso usarei o padrão '\\W' de Expressão Regular, que corresponde a caracteres diferentes de letras e algarismos, \n",
    "# ou seja, corresponde a espaços, pontuações.\n",
    "\n",
    "regex_tokenizer = RegexTokenizer(inputCol='value', outputCol='words',\n",
    "                                pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tokenized = regex_tokenizer.transform(text_task_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|               words|\n",
      "+--------------------+--------------------+\n",
      "|henDRERIt. MoNTEs...|[hendrerit, monte...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(value,StringType,true),StructField(words,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tokenized.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo meu contador de palavras, a ser utilizado em uma udf.\n",
    "\n",
    "def word_counter(lista,num_max=10):\n",
    "    #dicionário para contagem de palavras:\n",
    "    count_words = {}\n",
    "    \n",
    "    #variável para registrar número máximo de caracteres considerado na contagem:\n",
    "    key_max = 'MAIORES QUE ' + str(num_max)\n",
    "    \n",
    "    #contagem de palavras com mais de 'num_max' inicializada com 0\n",
    "    count_words = {key_max:0}\n",
    "    \n",
    "    #Para cada nova palavra encontrada, adiciono nova referência ao dicionário\n",
    "    #Para cada palavra repetida que aparece adiciono 1 ao contador:\n",
    "    for word in lista:\n",
    "        if len(word) <=num_max:\n",
    "            if word in count_words:\n",
    "                count_words[word] +=1\n",
    "                print(word)\n",
    "            else:\n",
    "                count_words[word] =1\n",
    "        else:\n",
    "            count_words[key_max] +=1\n",
    "    return [(key,value) for (key,value) in count_words.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = udf(word_counter, ArrayType(StructType([StructField(\"palavra\",StringType(),True),StructField(\"contagem\",IntegerType(), True)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = rg_tokenized.withColumn(\"palavras_contadas\", count_words(col(\"words\"))).select(\"palavras_contadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            palavras|\n",
      "+--------------------+\n",
      "|[MAIORES QUE 10, ...|\n",
      "|     [hendrerit, 21]|\n",
      "|        [montes, 22]|\n",
      "|         [purus, 21]|\n",
      "|        [luctus, 13]|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task1.show(1, False)\n",
    "\n",
    "# Forma de visualizar melhor o resultado.\n",
    "from pyspark.sql.functions import explode\n",
    "task1.withColumn(\"palavras\",\n",
    "                 explode(task1.palavras_contadas)\n",
    "                ).select('palavras').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1.write.json('palavras_contadas.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_task1 = task1.select(\"palavras_contadas\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras_contadas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(MAIORES QUE 10, 115), (hendrerit, 21), (mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   palavras_contadas\n",
       "0  [(MAIORES QUE 10, 115), (hendrerit, 21), (mont..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_task1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_task1.to_csv('task1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/57381557/pyspark-converting-an-array-of-struct-into-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tentativa fracassada de transformar a lista em uma string para contornar um problema\n",
    "# de exportar tipo Array diretamente para CSV.\n",
    "#def stringfier(lista):\n",
    "#    resultado = \"[\"\n",
    "#    for element in lista:\n",
    "#        resultado.append(\"[\"+str(element[0])+','+str(element[1])+']'+',')\n",
    "#    resultado.rstrip(',').append(']')\n",
    "#    return resultado\n",
    "\n",
    "#stringfy = udf(stringfier, StringType())\n",
    "#task1.withColumn('stringificado',stringfy(col(\"palavras_contadas\"))).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "- Leia o arquivo pedidos.csv \n",
    "- Agrupe todos os cliente que fizeram mais de 2 compras nos dias de black friday dos últimos três anos. \n",
    "- Filtre todos os clientes que são menores de 30 anos e coloque numa lista TODOS os códigos de pedido e a data em que foram efetuados. \n",
    "- Adicione também a idade do cliente. \n",
    "\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca Task2\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DateType)\n",
    "\n",
    "from pyspark.sql.functions import (to_date,\n",
    "                                   to_timestamp,\n",
    "                                   unix_timestamp,\n",
    "                                   from_unixtime,\n",
    "                                   lit,\n",
    "                                   current_date,\n",
    "                                   datediff,\n",
    "                                   collect_list,\n",
    "                                   arrays_zip,\n",
    "                                   concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(codigo_pedido,StringType,true),StructField(codigo_cliente,StringType,true),StructField(data_nascimento_cliente,StringType,true),StructField(data_pedido,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Leitura do Arquivo:\n",
    "url2 = 'https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv'\n",
    "\n",
    "spark.sparkContext.addFile(url2)\n",
    "\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True)\n",
    "\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/luizalabs/as-armadilhas-do-spark-101-d13a3296dcd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuração do Schema\n",
    "schema = StructType([\n",
    "    StructField(\"codigo_pedido\",StringType(),True),\n",
    "    StructField(\"codigo_cliente\",StringType(),True),\n",
    "    StructField(\"data_nascimento_cliente\",StringType(),True),\n",
    "    StructField(\"data_pedido\",StringType(),True)])\n",
    "\n",
    "#Nova leitura com o schema correto.\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_pedido: string (nullable = true)\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- data_nascimento_cliente: string (nullable = true)\n",
      " |-- data_pedido: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando o aspecto dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(data_nascimento_cliente='1985-12-04 00:00:00')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.select(\"data_nascimento_cliente\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(data_pedido='1542974527')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.select(\"data_pedido\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criarei uma nova coluna \"d_nascimento\" através da transformação da coluna \"data_nascimento\": transformando a informação do tipo string para o tipo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = table.withColumn('d_nascimento', to_date(table.data_nascimento_cliente).alias(\"to_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|d_nascimento|\n",
      "+------------+\n",
      "|  1985-12-04|\n",
      "|  1979-11-14|\n",
      "|  1989-07-25|\n",
      "|  1953-12-14|\n",
      "|  1985-05-03|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.select(\"d_nascimento\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora vou verificar o formato de \"data_pedido\" e transformar em um formato que permita a análise desejada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|         data_pedido|\n",
      "+-------+--------------------+\n",
      "|  count|              297309|\n",
      "|   mean|1.5287494219447646E9|\n",
      "| stddev|1.5643343589912213E7|\n",
      "|    min|          1509503453|\n",
      "|    max|          1543622000|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.select('data_pedido').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente \"data_pedido\" o tempo Unix: contagem em segundos com relação ao marco UNIX (1970-01-01 00:00:00 UTC).\n",
    "\n",
    "Referência: https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criarei uma nova coluna \"d_pedido\" através da transformação da coluna \"data_pedido\": transformando a informação unixtime (registrado como inteiro) para o tipo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = new_table.withColumn('d_pedido',to_date(from_unixtime(table.data_pedido)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  d_pedido|\n",
      "+----------+\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.select('d_pedido').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(codigo_pedido='bc8b03a005d5bf742fc7290db1b218de', codigo_cliente='b07af86a4a68707373856bcc3946583f', data_nascimento_cliente='1985-12-04 00:00:00', data_pedido='1542974527', d_nascimento=datetime.date(1985, 12, 4), d_pedido=datetime.date(2018, 11, 23))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_pedido: string (nullable = true)\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- data_nascimento_cliente: string (nullable = true)\n",
      " |-- data_pedido: string (nullable = true)\n",
      " |-- d_nascimento: date (nullable = true)\n",
      " |-- d_pedido: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora que já temos um formato adequado para tratar as datas dos pedidos precisamos filtrar os clientes que realizaram mais de 2 compras considerando as três últimas datas de Black Friday.\n",
    "\n",
    "Black friday dos últimos três anos:\n",
    "- 2018: 23 de novembro\n",
    "- 2017: 24 de novembro\n",
    "- 2016: 25 de novembro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_fridays = (\"2018-11-23\", \"2017-11-24\", \"2016-11-25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contando pedidos feitos nas datas de Black Friday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contagem_black = new_table.filter((new_table.d_pedido == (lit(black_fridays[0])))|(new_table.d_pedido ==lit(black_fridays[1]))|(new_table.d_pedido ==lit(black_fridays[2]))).groupBy('codigo_cliente').agg({'codigo_pedido':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- count(codigo_pedido): long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contagem_black.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codigo_cliente', 'count(codigo_pedido)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contagem_black.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrando clientes que realizaram mais de duas compras considerando as três últimas Black Fridays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_black = contagem_black.filter(contagem_black['count(codigo_pedido)']>2).select('codigo_cliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      codigo_cliente|\n",
      "+--------------------+\n",
      "|b67ef7abecc0a8e88...|\n",
      "|28688f66084a7f1de...|\n",
      "|3f89d915a06a3d01e...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes_black.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tendo a lista de clientes compradores que satisfazem a condição de ter feito mais de duas compras ao longo das últimas três Black Fridays, conseguimos recuperar a totalidade de compras realizadas por eles (tanto dentro como fora do período de Black Friday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizando a junção interna da tabela de clientes com a tabela contendo o registro geral de pedidos para filtrar \n",
    "# apenas os clientes que satisfazem a condição e resgatar as demais compras feitas por eles.\n",
    "compras_clientes_black = clientes_black.join(new_table, clientes_black.codigo_cliente == new_table.codigo_cliente, \"inner\")\n",
    "\n",
    "#retirando coluna duplicada vinda do join\n",
    "compras_clientes_black = compras_clientes_black.drop(new_table[\"codigo_cliente\"])\n",
    "\n",
    "#retirando registros duplicados vindos do join\n",
    "compras_clientes_black.dropDuplicates()\n",
    "\n",
    "#contando o número de pedidos feitos pelos clientes (tanto na Black Friday como fora dela)\n",
    "# e armazenando em compra_geral_clientes_black\n",
    "compra_geral_clientes_black = compras_clientes_black.groupBy('codigo_cliente').agg({'codigo_pedido':'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devemos agora construir uma nova coluna \"idade\" a partir da coluna \"d_nascimento\" para selecionar apenas os clientes de menos de 30 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A coluna \"idade\" é construída pela diferença entre a data atual e a data\n",
    "# do aniversário registrada em 'd_nascimento'.\n",
    "compras_clientes_black = compras_clientes_black.withColumn('idade', ((datediff(current_date(),compras_clientes_black.d_nascimento))/365).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|idade|\n",
      "+-----+\n",
      "|   49|\n",
      "|   49|\n",
      "|    8|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compras_clientes_black.select('idade').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrando clientes que satisfazem a condição de compra e que têm menos de 30 anos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black_under30 = compras_clientes_black.filter(compras_clientes_black['idade']<30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/37440373/spark-dataframe-aggregate-column-values-by-key-into-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui construirei a tabela \"pedido\" que registra para cada cliente uma lista com TODOS os pedidos no \n",
    "# formato [\"código\",\"data\"] dos clientes filtrados.\n",
    "\n",
    "pedido = (compras_clientes_black_under30.groupBy(compras_clientes_black_under30[\"codigo_cliente\"])\n",
    "     .agg(collect_list(\"codigo_pedido\").alias(\"pedidos\"),\n",
    "        collect_list(\"d_pedido\").alias(\"data\"))\n",
    "     .withColumn(\"pedidos\", arrays_zip(\"pedidos\",\"data\")).drop(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      codigo_cliente|             pedidos|\n",
      "+--------------------+--------------------+\n",
      "|3bfcd49a281054bbf...|[[a071669a5000ce0...|\n",
      "|18d6c857acbfb21b5...|[[aede83e21d3683a...|\n",
      "|3860f681456fae15d...|[[66c1ea9f1aa3b25...|\n",
      "|a3c3bc3ebac103447...|[[07089f306713afa...|\n",
      "|d221905b7af7b56c0...|[[82c2a7a825ac040...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedido.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|      codigo_cliente|count(codigo_pedido)|idade|             pedidos|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|3bfcd49a281054bbf...|                   3|   19|[[a071669a5000ce0...|\n",
      "|18d6c857acbfb21b5...|                   4|   29|[[aede83e21d3683a...|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agora construirei a visualização solicitada:\n",
    "\n",
    "# Unindo a tabela contendo informações gerais com a contendo a lista de pedidos de cada cliente filtrado:\n",
    "visual = compras_clientes_black_under30.join(pedido, compras_clientes_black_under30.codigo_cliente == pedido.codigo_cliente,\"inner\").drop(pedido[\"codigo_cliente\"])\n",
    "# Unindo agora com o registro de contagem de compras gerais feitas pelo cliente filtrado\n",
    "visual = visual.join(compra_geral_clientes_black, visual.codigo_cliente == compra_geral_clientes_black.codigo_cliente).drop(compra_geral_clientes_black[\"codigo_cliente\"])\n",
    "# Selecionando apenas as colunas solicitadas para a visualização\n",
    "visual = visual.select(['codigo_cliente','count(codigo_pedido)','idade','pedidos'])\n",
    "# deduplicando os registros duplicados advindos do join\n",
    "visual = visual.dropDuplicates()\n",
    "\n",
    "# apenas para verificação:\n",
    "visual.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pedidos=[Row(pedidos='a071669a5000ce07ad709886c8109906', data=datetime.date(2018, 11, 23)), Row(pedidos='ca2360f9f20482f1f0d851a15ae67498', data=datetime.date(2018, 11, 23)), Row(pedidos='cb422c641730320669dc49b88c069a00', data=datetime.date(2018, 11, 23))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual.select('pedidos').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(codigo_cliente,StringType,true),StructField(count(codigo_pedido),LongType,false),StructField(idade,IntegerType,true),StructField(pedidos,ArrayType(StructType(List(StructField(pedidos,StringType,true),StructField(data,DateType,true))),false),true)))\n"
     ]
    }
   ],
   "source": [
    "print(visual.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual.write.json('pedidos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_pandas = visual.toPandas()\n",
    "visual_pandas.to_csv('task2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
