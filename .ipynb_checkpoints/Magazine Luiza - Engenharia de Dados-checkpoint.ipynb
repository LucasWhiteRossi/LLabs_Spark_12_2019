{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio: Engenharia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "Leia o arquivo de texto ​ wordcount.txt ​ e conte as palavras que contém até 10 letras. Conte\n",
    "também quantas palavras com mais de 10 letras existem no texto.\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt\n",
    "Exemplo do dataset final:\n",
    "[('two', 2),\n",
    "('behold', 1),\n",
    "('itself', 3),\n",
    "(‘MAIORES QUE 10’, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/lucas/spark-3.0.0-preview-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca Task1\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               IntegerType, \n",
    "                               ArrayType, \n",
    "                               StringType,\n",
    "                               StructField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando sessão\n",
    "spark = SparkSession.builder.appName('desafio').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(value,StringType,true)))\n",
      "\n",
      "\n",
      "['value']\n"
     ]
    }
   ],
   "source": [
    "# Leitura do texto\n",
    "\n",
    "url1 = 'https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt'\n",
    "\n",
    "spark.sparkContext.addFile(url1)\n",
    "\n",
    "text_task_1 = spark.read.text(\"file://\" + SparkFiles.get('wordcount.txt'), wholetext=True, lineSep=None)\n",
    "\n",
    "print(text_task_1.schema)\n",
    "#print(text_task_1.head())\n",
    "print('\\n')\n",
    "print(text_task_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretendo eliminar todo o tipo de pontuação, espaço ou símbolo do texto.\n",
    "# O padrão '\\W' de Expressão Regular corresponde a caracteres diferentes de letras e algarismos, \n",
    "# ou seja, corresponde a espaços, pontuações e caracteres especiais\n",
    "regex_tokenizer = RegexTokenizer(inputCol='value', outputCol='words',\n",
    "                                pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tokenized = regex_tokenizer.transform(text_task_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|               words|\n",
      "+--------------------+--------------------+\n",
      "|henDRERIt. MoNTEs...|[hendrerit, monte...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(value,StringType,true),StructField(words,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_tokenized.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(lista,num_max=10):\n",
    "    count_words = {}\n",
    "    key_max = 'MAIORES QUE ' + str(num_max)\n",
    "    count_words = {key_max:0}\n",
    "    for word in lista:\n",
    "        if len(word) <=num_max:\n",
    "            if word in count_words:\n",
    "                count_words[word] +=1\n",
    "                print(word)\n",
    "            else:\n",
    "                count_words[word] =1\n",
    "        else:\n",
    "            count_words[key_max] +=1\n",
    "    return [(key,value) for (key,value) in count_words.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = udf(word_counter, ArrayType(StructType([StructField(\"palavra\",StringType(),True),StructField(\"contagem\",IntegerType(), True)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = rg_tokenized.withColumn(\"palavras_contadas\", count_words(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|palavras_contadas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[MAIORES QUE 10, 115], [hendrerit, 21], [montes, 22], [purus, 21], [luctus, 13], [dictum, 15], [est, 11], [mattis, 11], [phasellus, 8], [dignissim, 10], [rhoncus, 18], [cubilia, 15], [sit, 19], [nunc, 13], [at, 14], [interdum, 18], [vivamus, 22], [ante, 18], [ac, 14], [amet, 12], [elementum, 8], [congue, 12], [justo, 11], [eget, 16], [egestas, 16], [vitae, 16], [per, 12], [lacus, 10], [sociis, 17], [quisque, 21], [eleifend, 13], [inceptos, 18], [etiam, 21], [vel, 18], [augue, 9], [proin, 9], [placerat, 18], [conubia, 16], [odio, 20], [feugiat, 13], [venenatis, 18], [class, 10], [ultrices, 10], [convallis, 17], [aenean, 12], [diam, 20], [donec, 17], [in, 14], [neque, 12], [nisl, 8], [semper, 16], [netus, 17], [porttitor, 17], [aliquet, 10], [eros, 14], [viverra, 13], [sodales, 14], [integer, 17], [auctor, 10], [tortor, 10], [ultricies, 13], [posuere, 12], [ipsum, 10], [quam, 11], [sociosqu, 11], [lectus, 15], [nascetur, 25], [eu, 14], [nec, 15], [tempor, 11], [sagittis, 13], [rutrum, 17], [facilisis, 15], [turpis, 12], [platea, 11], [ut, 11], [ligula, 12], [sem, 10], [fringilla, 12], [dictumst, 9], [malesuada, 13], [arcu, 18], [cum, 13], [potenti, 11], [nibh, 12], [penatibus, 13], [orci, 18], [commodo, 9], [nulla, 7], [morbi, 14], [pulvinar, 14], [magnis, 15], [massa, 8], [natoque, 17], [faucibus, 12], [a, 19], [dapibus, 17], [volutpat, 14], [facilisi, 8], [fermentum, 20], [nisi, 8], [sapien, 18], [dolor, 15], [metus, 8], [laoreet, 22], [mi, 10], [pharetra, 11], [vehicula, 15], [suscipit, 13], [velit, 12], [habitant, 21], [litora, 15], [vestibulum, 12], [curae, 18], [dui, 16], [urna, 19], [ad, 14], [tristique, 11], [pede, 11], [nullam, 13], [varius, 12], [aliquam, 14], [porta, 13], [non, 13], [cras, 9], [pretium, 11], [hac, 15], [blandit, 12], [hymenaeos, 13], [tellus, 20], [enim, 17], [tempus, 20], [euismod, 8], [tincidunt, 10], [cursus, 12], [et, 14], [lacinia, 13], [dis, 17], [mus, 10], [accumsan, 10], [torquent, 13], [habitasse, 14], [primis, 7], [elit, 13], [bibendum, 11], [leo, 17], [taciti, 13], [magna, 5], [imperdiet, 11], [erat, 13], [parturient, 10], [mauris, 11], [fusce, 7], [lorem, 13], [sed, 10], [praesent, 21], [ridiculus, 9], [felis, 11], [senectus, 11], [mollis, 17], [lobortis, 7], [adipiscing, 14], [iaculis, 9], [vulputate, 9], [aptent, 11], [maecenas, 14], [ornare, 7], [quis, 9], [molestie, 10], [id, 11], [fames, 8], [gravida, 5], [consequat, 11], [curabitur, 10], [libero, 11], [nonummy, 12], [duis, 18], [risus, 12], [nam, 10], [nostra, 9]]|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task1.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/57381557/pyspark-converting-an-array-of-struct-into-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringfier(lista):\n",
    "    resultado = \"[\"\n",
    "    for element in lista:\n",
    "        resultado.append(\"[\"+str(element[0])+','+str(element[1])+']'+',')\n",
    "    resultado.rstrip(',').append(']')\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringfy = udf(stringfier, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o544.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16, 10.0.2.15, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 598, in main\n    process()\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 590, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 465, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 79, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-87-270a9a03ddc9>\", line 4, in stringfier\nAttributeError: 'str' object has no attribute 'append'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:447)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3417)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2516)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3407)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3403)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2516)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2723)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 598, in main\n    process()\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 590, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 465, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 79, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-87-270a9a03ddc9>\", line 4, in stringfier\nAttributeError: 'str' object has no attribute 'append'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6bb79de231c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stringificado'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstringfy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"palavras_contadas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o544.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16, 10.0.2.15, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 598, in main\n    process()\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 590, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 465, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 79, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-87-270a9a03ddc9>\", line 4, in stringfier\nAttributeError: 'str' object has no attribute 'append'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:447)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3417)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2516)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3407)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3403)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2516)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2723)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 598, in main\n    process()\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 590, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 465, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 454, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/lucas/spark-3.0.0-preview-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 79, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-87-270a9a03ddc9>\", line 4, in stringfier\nAttributeError: 'str' object has no attribute 'append'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "task1.withColumn('stringificado',stringfy(col(\"palavras_contadas\"))).show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import explode\n",
    "#task1.withColumn(\"palavras\", explode(task1.palavras_contadas)).select('palavras').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support array<struct<palavra:string,contagem:int>> data type.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o303.csv.\n: org.apache.spark.sql.AnalysisException: CSV data source does not support array<struct<palavra:string,contagem:int>> data type.;\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$verifySchema$1(DataSourceUtils.scala:54)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$verifySchema$1$adapted(DataSourceUtils.scala:52)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:52)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:129)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:175)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:105)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:103)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:124)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:227)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:224)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:829)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:829)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:309)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:819)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-a18f4b1d3d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'palavras_contadas.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    946\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-preview-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support array<struct<palavra:string,contagem:int>> data type.;"
     ]
    }
   ],
   "source": [
    "task1.write.csv('palavras_contadas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('words_list.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(objective1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "- Leia o arquivo pedidos.csv \n",
    "- Agrupe todos os cliente que fizeram mais de 2 compras nos dias de black friday dos últimos três anos. \n",
    "- Filtre todos os clientes que são menores de 30 anos e coloque numa lista TODOS os códigos de pedido e a data em que foram efetuados. \n",
    "- Adicione também a idade do cliente. \n",
    "\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca Task2\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DateType)\n",
    "\n",
    "from pyspark.sql.functions import (to_date,\n",
    "                                   to_timestamp,\n",
    "                                   unix_timestamp,\n",
    "                                   from_unixtime,\n",
    "                                   lit,\n",
    "                                   current_date,\n",
    "                                   datediff,\n",
    "                                   collect_list,\n",
    "                                   arrays_zip,\n",
    "                                   concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(codigo_pedido,StringType,true),StructField(codigo_cliente,StringType,true),StructField(data_nascimento_cliente,StringType,true),StructField(data_pedido,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Leitura do Arquivo:\n",
    "url2 = 'https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv'\n",
    "\n",
    "spark.sparkContext.addFile(url2)\n",
    "\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True)\n",
    "\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/luizalabs/as-armadilhas-do-spark-101-d13a3296dcd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuração do Schema\n",
    "schema = StructType([\n",
    "    StructField(\"codigo_pedido\",StringType(),True),\n",
    "    StructField(\"codigo_cliente\",StringType(),True),\n",
    "    StructField(\"data_nascimento_cliente\",StringType(),True),\n",
    "    StructField(\"data_pedido\",StringType(),True)])\n",
    "\n",
    "#Nova leitura com o schema correto.\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_pedido: string (nullable = true)\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- data_nascimento_cliente: string (nullable = true)\n",
      " |-- data_pedido: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando o aspecto dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(data_nascimento_cliente='1985-12-04 00:00:00')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.select(\"data_nascimento_cliente\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(data_pedido='1542974527')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.select(\"data_pedido\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criarei uma nova coluna \"d_nascimento\" através da transformação da coluna \"data_nascimento\": transformando a informação do tipo string para o tipo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = table.withColumn('d_nascimento', to_date(table.data_nascimento_cliente).alias(\"to_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|d_nascimento|\n",
      "+------------+\n",
      "|  1985-12-04|\n",
      "|  1979-11-14|\n",
      "|  1989-07-25|\n",
      "|  1953-12-14|\n",
      "|  1985-05-03|\n",
      "|  1980-04-16|\n",
      "|  1991-11-18|\n",
      "|  1974-01-04|\n",
      "|  1985-04-18|\n",
      "|  1981-09-15|\n",
      "|  1996-04-17|\n",
      "|  1970-11-15|\n",
      "|  1964-10-12|\n",
      "|  1990-06-25|\n",
      "|  1979-09-30|\n",
      "|  1958-05-13|\n",
      "|  1976-05-18|\n",
      "|  1988-01-13|\n",
      "|  1993-08-10|\n",
      "|  1978-11-20|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.select(\"d_nascimento\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora vou verificar o formato de \"data_pedido\" e transformar em um formato que permita a análise desejada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|         data_pedido|\n",
      "+-------+--------------------+\n",
      "|  count|              297309|\n",
      "|   mean|1.5287494219447646E9|\n",
      "| stddev|1.5643343589912213E7|\n",
      "|    min|          1509503453|\n",
      "|    max|          1543622000|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.select('data_pedido').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente \"data_pedido\" o tempo Unix: contagem em segundos com relação ao marco UNIX (1970-01-01 00:00:00 UTC).\n",
    "\n",
    "Referência: https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criarei uma nova coluna \"d_pedido\" através da transformação da coluna \"data_pedido\": transformando a informação unixtime (registrado como inteiro) para o tipo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = new_table.withColumn('d_pedido',to_date(from_unixtime(table.data_pedido)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  d_pedido|\n",
      "+----------+\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "|2018-11-23|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.select('d_pedido').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(codigo_pedido='bc8b03a005d5bf742fc7290db1b218de', codigo_cliente='b07af86a4a68707373856bcc3946583f', data_nascimento_cliente='1985-12-04 00:00:00', data_pedido='1542974527', d_nascimento=datetime.date(1985, 12, 4), d_pedido=datetime.date(2018, 11, 23))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_pedido: string (nullable = true)\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- data_nascimento_cliente: string (nullable = true)\n",
      " |-- data_pedido: string (nullable = true)\n",
      " |-- d_nascimento: date (nullable = true)\n",
      " |-- d_pedido: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora que já temos um formato adequado para tratar as datas dos pedidos precisamos filtrar os clientes que realizaram mais de 2 compras considerando as três últimas datas de Black Friday.\n",
    "\n",
    "Black friday dos últimos três anos:\n",
    "- 2018: 23 de novembro\n",
    "- 2017: 24 de novembro\n",
    "- 2016: 25 de novembro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_fridays = (\"2018-11-23\", \"2017-11-24\", \"2016-11-25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtrando dias de pedido correspondentes à Black Friday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_table.filter((new_table.d_pedido == (lit(black_fridays[0])))|(new_table.d_pedido ==lit(black_fridays[1]))|(new_table.d_pedido ==lit(black_fridays[2]))).groupBy('codigo_cliente').agg({'codigo_pedido':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- codigo_cliente: string (nullable = true)\n",
      " |-- count(codigo_pedido): long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codigo_cliente', 'count(codigo_pedido)']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrando clientes que realizaram mais de duas compras considerando as três últimas Black Fridays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_black = result.filter(result['count(codigo_pedido)']>2).select('codigo_cliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      codigo_cliente|\n",
      "+--------------------+\n",
      "|b67ef7abecc0a8e88...|\n",
      "|28688f66084a7f1de...|\n",
      "|3f89d915a06a3d01e...|\n",
      "|c371799c2befffb67...|\n",
      "|e7c52e68263476a2d...|\n",
      "|3bfcd49a281054bbf...|\n",
      "|f184a197ec54c7eb1...|\n",
      "|1375a4e01d4811249...|\n",
      "|3531fd9696a342b74...|\n",
      "|4cf0d3732731a0653...|\n",
      "|c512a4d48ee9388e1...|\n",
      "|057ae5d7ef3fcdd74...|\n",
      "|d2b6a6676c81b7e84...|\n",
      "|e40809b3ff1805b54...|\n",
      "|c3f1cb573d199ae93...|\n",
      "|18d6c857acbfb21b5...|\n",
      "|3860f681456fae15d...|\n",
      "|fcf0d288e3488a3ab...|\n",
      "|01b9f95dce03e2382...|\n",
      "|81ba3bb6df12e897c...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientes_black.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tendo a lista de clientes compradores que satisfazem a condição de ter feito mais de duas compras nas últimas três Black Fridays, conseguimos recuperar a totalidade de compras realizadas por eles (tanto dentro como fora do período de Black Friday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black = clientes_black.join(new_table, clientes_black.codigo_cliente == new_table.codigo_cliente, \"inner\")\n",
    "compras_clientes_black = compras_clientes_black.drop(new_table[\"codigo_cliente\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devemos agora construir uma nova coluna \"idade\" a partir da coluna \"d_nascimento\" para selecionar apenas os clientes de menos de 30 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A coluna \"idade\" é construída pela diferença entre a data atual e a data\n",
    "# do aniversário registrada em 'd_nascimento'.\n",
    "compras_clientes_black = compras_clientes_black.withColumn('idade', ((datediff(current_date(),compras_clientes_black.d_nascimento))/365).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|idade|\n",
      "+-----+\n",
      "|   49|\n",
      "|   49|\n",
      "|    8|\n",
      "|   49|\n",
      "|   49|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compras_clientes_black.select('idade').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrando clientes que satisfazem a condição de compra e que têm menos de 30 anos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black_under30 = compras_clientes_black.filter(compras_clientes_black['idade']<30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/37440373/spark-dataframe-aggregate-column-values-by-key-into-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black_under30 = compras_clientes_black_under30.withColumn(\"d_pedido_2\", compras_clientes_black_under30.d_pedido.cast(StringType())).drop(\"d_pedido\").withColumnRenamed(\"d_pedido_2\", \"d_pedido\")\n",
    "\n",
    "\n",
    "pedido = (compras_clientes_black_under30.groupBy(compras_clientes_black_under30[\"codigo_cliente\"])\n",
    "     .agg(collect_list(\"codigo_pedido\").alias(\"pedidos\"),\n",
    "        collect_list(\"d_pedido\").alias(\"data\"))\n",
    "     .withColumn(\"pedidos\", arrays_zip(\"pedidos\",\"data\")).drop(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      codigo_cliente|             pedidos|\n",
      "+--------------------+--------------------+\n",
      "|3bfcd49a281054bbf...|[[a071669a5000ce0...|\n",
      "|18d6c857acbfb21b5...|[[aede83e21d3683a...|\n",
      "|3860f681456fae15d...|[[66c1ea9f1aa3b25...|\n",
      "|a3c3bc3ebac103447...|[[07089f306713afa...|\n",
      "|d221905b7af7b56c0...|[[82c2a7a825ac040...|\n",
      "|4112bda7877566650...|[[08d28852abb0610...|\n",
      "|047f50ac99e51749f...|[[89419f4a8b4d993...|\n",
      "|70a35a3d4743b9503...|[[88bebb01617d795...|\n",
      "|22275184edd41a8a9...|[[44328873b865655...|\n",
      "|3224ac7dee3bd6196...|[[110fdb292cc6b2c...|\n",
      "|a61b2a2669b043efb...|[[4a499631d7360c3...|\n",
      "|e357a3944a07df38f...|[[9ebfc7efabf4d41...|\n",
      "|77547c27288c4114e...|[[adbdd9fb05ecd13...|\n",
      "|9436b0e2e66029690...|[[eb9f57c8fd92752...|\n",
      "|6ac4d4625eaaf12da...|[[d2973feaa1be74d...|\n",
      "|48bd44c049ba3ff9d...|[[122fb9734cb8e79...|\n",
      "|d639cb78545b7c00d...|[[787f7bb91d3e155...|\n",
      "|76fa9d1303b605161...|[[df5b474943a7c14...|\n",
      "|e6302fd4bc1b9a8c8...|[[9b17d4b5254b8d0...|\n",
      "|9ba43b1b796103b4f...|[[ae5f74bbf632921...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pedido.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|      codigo_cliente|count(codigo_pedido)|idade|             pedidos|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|3bfcd49a281054bbf...|                   3|   19|[[a071669a5000ce0...|\n",
      "|18d6c857acbfb21b5...|                   3|   29|[[aede83e21d3683a...|\n",
      "|3860f681456fae15d...|                   3|   19|[[66c1ea9f1aa3b25...|\n",
      "|a3c3bc3ebac103447...|                   3|    9|[[07089f306713afa...|\n",
      "|d221905b7af7b56c0...|                   3|   27|[[82c2a7a825ac040...|\n",
      "|4112bda7877566650...|                   3|   23|[[08d28852abb0610...|\n",
      "|047f50ac99e51749f...|                   3|   25|[[89419f4a8b4d993...|\n",
      "|70a35a3d4743b9503...|                   3|   23|[[88bebb01617d795...|\n",
      "|22275184edd41a8a9...|                   3|   25|[[44328873b865655...|\n",
      "|3224ac7dee3bd6196...|                   3|   19|[[110fdb292cc6b2c...|\n",
      "|a61b2a2669b043efb...|                   3|   23|[[4a499631d7360c3...|\n",
      "|e357a3944a07df38f...|                   3|   26|[[9ebfc7efabf4d41...|\n",
      "|77547c27288c4114e...|                   3|   28|[[adbdd9fb05ecd13...|\n",
      "|9436b0e2e66029690...|                 118|   10|[[eb9f57c8fd92752...|\n",
      "|6ac4d4625eaaf12da...|                   3|   27|[[d2973feaa1be74d...|\n",
      "|48bd44c049ba3ff9d...|                   3|   28|[[122fb9734cb8e79...|\n",
      "|d639cb78545b7c00d...|                   3|   26|[[787f7bb91d3e155...|\n",
      "|76fa9d1303b605161...|                   3|   22|[[df5b474943a7c14...|\n",
      "|e6302fd4bc1b9a8c8...|                   3|   19|[[9b17d4b5254b8d0...|\n",
      "|9ba43b1b796103b4f...|                 737|   18|[[ae5f74bbf632921...|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual = compras_clientes_black_under30.join(pedido, compras_clientes_black_under30.codigo_cliente == pedido.codigo_cliente,\"inner\").drop(pedido[\"codigo_cliente\"])\n",
    "#\n",
    "visual = visual.withColumn(\"d_pedido_2\", visual.d_pedido.cast(StringType())).drop(\"d_pedido\").withColumnRenamed(\"d_pedido_2\", \"d_pedido\")\n",
    "#\n",
    "visual = visual.join(result, visual.codigo_cliente == result.codigo_cliente).drop(result[\"codigo_cliente\"])\n",
    "visual = visual.select(['codigo_cliente','count(codigo_pedido)','idade','pedidos'])\n",
    "visual = visual.dropDuplicates()\n",
    "visual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pedidos                                                                                                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[a071669a5000ce07ad709886c8109906, 2018-11-23], [ca2360f9f20482f1f0d851a15ae67498, 2018-11-23], [cb422c641730320669dc49b88c069a00, 2018-11-23]]                                                |\n",
      "|[[aede83e21d3683a716106a85d208ded9, 2018-11-23], [fa32e1d7d905159afec553b506dc4394, 2018-11-23], [3a6abfc49a47d5d30d06747b5e781681, 2018-11-23], [b9f4d17e40a03154ada5c1ed1a1d87f4, 2017-11-11]]|\n",
      "|[[66c1ea9f1aa3b2544d5e9ba911c0f790, 2018-11-23], [4cc754634d81b8dcfb51a7f7ae949d19, 2018-11-23], [41248accd291f46a50b0cdba30adf7c9, 2018-11-23]]                                                |\n",
      "|[[07089f306713afa35b6541fa51e87198, 2018-11-23], [b0b219b15cca20027622de7d210f632f, 2018-11-23], [8420964c04d564e625d051c2f8862565, 2018-11-23]]                                                |\n",
      "|[[82c2a7a825ac0401d8ca18f58bc76a08, 2018-11-23], [a0cd825d3d69a19db8d644fe91d29dc3, 2018-11-23], [5bd8524dac7e62b57412a0c28bbe4cfc, 2018-11-23], [6b139d0dc998515629dad0a26addcd14, 2018-11-08]]|\n",
      "|[[08d28852abb06109ab53648ed41411bc, 2018-11-23], [ebc3616ec4584ba4d66c85a6b4d2b1e6, 2018-11-23], [76c9b99f16e98b70433fd9f5acfe9630, 2018-11-23]]                                                |\n",
      "|[[89419f4a8b4d9938a1ec0137b0770155, 2017-11-24], [771c2494418c023e6ab3d2db396428b9, 2017-11-24], [c7a1196759203ea33fa72c4a9c606802, 2017-11-24]]                                                |\n",
      "|[[88bebb01617d7950ac1777f8ecb0f8a1, 2018-11-23], [81ca72a9af13fa7b5620081694c62ac0, 2018-11-23], [40d50ecd23109b800e6d4a0524bfca3c, 2018-11-23]]                                                |\n",
      "|[[44328873b8656556e13d1dcffbaead08, 2018-11-23], [18d825f3c2211953aa4fa0d6d7b3a8ab, 2018-11-23], [b422a85e4ac2f418313f5a6777171595, 2018-11-23]]                                                |\n",
      "|[[110fdb292cc6b2cc185578492b12558c, 2018-11-23], [b0ec0e67b4092ffc2de13561c47074be, 2018-11-23], [715fd96b0daffb567c062f6a4c769345, 2018-11-23]]                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual.select('pedidos').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(codigo_cliente,StringType,true),StructField(count(codigo_pedido),LongType,false),StructField(idade,IntegerType,true),StructField(pedidos,ArrayType(StructType(List(StructField(pedidos,StringType,true),StructField(data,StringType,true))),false),true)))\n"
     ]
    }
   ],
   "source": [
    "print(visual.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual.select('pedidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import udf, col\n",
    "\n",
    "#join_udf = udf(lambda x: \"[\" + \",\".join(str(x)) + \"]\", StringType())\n",
    "#visual.withColumn(\"pedidos_str\", join_udf(visual.pedidos)).show()\n",
    "\n",
    "#visual = visual.drop(visual['pedidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual.write.json('pedidos.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
