{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio: Engenharia de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "Leia o arquivo de texto ​ wordcount.txt ​ e conte as palavras que contém até 10 letras. Conte\n",
    "também quantas palavras com mais de 10 letras existem no texto.\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt\n",
    "Exemplo do dataset final:\n",
    "[('two', 2),\n",
    "('behold', 1),\n",
    "('itself', 3),\n",
    "(‘MAIORES QUE 10’, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10cac1ad0b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Biblioteca Task1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Biblioteca Task1\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('desafio').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do texto\n",
    "url1 = 'https://storage.googleapis.com/luizalabs-hiring-test/wordcount.txt'\n",
    "r = requests.get(url1)\n",
    "text_task_1 = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho do texto: {} caracteres \\n\".format(len(text_task_1)))\n",
    "# Para visualizar um pedaço do arquivo\n",
    "print(text_task_1[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    ext = re.sub('\\\\t','',text)\n",
    "    text = re.sub('\\\\n','',text)\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalhe: neste caso estou considerando que a identidade da palavra\n",
    "# não é case sensitive, ou seja, \"Ou\" é igual a \"OU\" e \"oU\".\n",
    "clean_text = remove_punctuation(text_task_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(lista,num_max):\n",
    "    count_words = {}\n",
    "    key_max = 'MAIORES QUE ' + str(num_max)\n",
    "    count_words = {key_max:0}\n",
    "    for word in lista:\n",
    "        if len(word) <=num_max:\n",
    "            if word in count_words:\n",
    "                count_words[word] +=1\n",
    "            else:\n",
    "                count_words[word] =1\n",
    "        else:\n",
    "            count_words[key_max] +=1\n",
    "    return [(key,value) for (key,value) in count_words.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective1 = sorted(word_counter(word_list,10), key= lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('words_list.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(objective1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "- Leia o arquivo pedidos.csv \n",
    "- Agrupe todos os cliente que fizeram mais de 2 compras nos dias de black friday dos últimos três anos. \n",
    "- Filtre todos os clientes que são menores de 30 anos e coloque numa lista TODOS os códigos de pedido e a data em que foram efetuados. \n",
    "- Adicione também a idade do cliente. \n",
    "\n",
    "Dataset: ​ https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Leitura do Arquivo:\n",
    "url2 = 'https://storage.googleapis.com/luizalabs-hiring-test/clientes_pedidos.csv'\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\n",
    "\n",
    "spark.sparkContext.addFile(url2)\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True)\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/luizalabs/as-armadilhas-do-spark-101-d13a3296dcd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"codigo_pedido\",StringType(),True),\n",
    "    StructField(\"codigo_cliente\",StringType(),True),\n",
    "    StructField(\"data_nascimento_cliente\",StringType(),True),\n",
    "    StructField(\"data_pedido\",StringType(),True)])\n",
    "\n",
    "table = spark.read.csv(\"file://\" + SparkFiles.get('clientes_pedidos.csv'), header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.select(\"data_nascimento_cliente\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.select(\"data_pedido\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (to_date,to_timestamp,\n",
    "                                    unix_timestamp,\n",
    "                                  from_unixtime,\n",
    "                                  lit, current_date, datediff,\n",
    "                                  collect_list, arrays_zip,\n",
    "                                  concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = table.withColumn('d_nascimento', to_date(table.data_nascimento_cliente).alias(\"to_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table.select(\"d_nascimento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-48fc3b8d2e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_pedido'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "table.select('data_pedido').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referência: https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/\n",
    "\n",
    "Aparentemente estamos lidando com a contagem em segundos com relação ao marco UNIX (1970-01-01 00:00:00 UTC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = new_table.withColumn('d_pedido',to_date(from_unixtime(table.data_pedido)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table.select('d_pedido').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black friday dos últimos três anos:\n",
    "- 2018: 23 de novembro\n",
    "- 2017: 24 de novembro\n",
    "- 2016: 25 de novembro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_fridays = (\"2018-11-23\", \"2017-11-24\", \"2016-11-25\")\n",
    "\n",
    "# filtrando dias de pedido correspondentes à Black Friday\n",
    "result = new_table.filter((new_table.d_pedido == (lit(black_fridays[0])))|(new_table.d_pedido ==lit(black_fridays[1]))|(new_table.d_pedido ==lit(black_fridays[2]))).groupBy('codigo_cliente').agg({'codigo_pedido':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_black = result.filter(result['count(codigo_pedido)']>2).select('codigo_cliente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_black.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restringindo os clientes aos que fizeram mais de 2 compras ao longo das três últimas Black Fridays \n",
    "\n",
    "compras_clientes_black = clientes_black.join(new_table, clientes_black.codigo_cliente == new_table.codigo_cliente, \"inner\")\n",
    "compras_clientes_black = compras_clientes_black.drop(new_table[\"codigo_cliente\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black = compras_clientes_black.withColumn('idade', ((datediff(current_date(),compras_clientes_black.d_nascimento))/365).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black.select('idade').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black_under30 = compras_clientes_black.filter(compras_clientes_black['idade']<30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/37440373/spark-dataframe-aggregate-column-values-by-key-into-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compras_clientes_black_under30 = compras_clientes_black_under30.withColumn(\"d_pedido_2\", compras_clientes_black_under30.d_pedido.cast(StringType())).drop(\"d_pedido\").withColumnRenamed(\"d_pedido_2\", \"d_pedido\")\n",
    "\n",
    "\n",
    "pedido = (compras_clientes_black_under30.groupBy(compras_clientes_black_under30[\"codigo_cliente\"])\n",
    "     .agg(collect_list(\"codigo_pedido\").alias(\"pedidos\"),\n",
    "        collect_list(\"d_pedido\").alias(\"data\"))\n",
    "     .withColumn(\"pedidos\", arrays_zip(\"pedidos\",\"data\")).drop(\"data\"))\n",
    "\n",
    "\n",
    "\n",
    "#pedido = (compras_clientes_black_under30.groupBy(compras_clientes_black_under30[\"codigo_cliente\"])\n",
    "#     .agg(collect_list(\"codigo_pedido\").alias(\"pedidos\"),\n",
    "#        collect_list(\"d_pedido\").alias(\"data\"))\n",
    "#     .withColumn(\"pedidos\", arrays_zip(\"pedidos\",\"data\")).drop(\"data\"))\n",
    "\n",
    "\n",
    "#(df\n",
    "#  .groupby(\"id\")\n",
    "#  .agg(F.collect_set(\"code\"),\n",
    "#       F.collect_list(\"name\"))\n",
    "#  .show())\n",
    "\n",
    "#df2 = df.groupBy(\"name\").agg(\n",
    "#  collect_list(col(\"food\")) as \"food\",\n",
    "#  collect_list(col(\"price\")) as \"price\" \n",
    "#).withColumn(\"food\", zipper(col(\"food\"), col(\"price\"))).drop(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pedido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "visual = compras_clientes_black_under30.join(pedido, compras_clientes_black_under30.codigo_cliente == pedido.codigo_cliente,\"inner\").drop(pedido[\"codigo_cliente\"])\n",
    "#\n",
    "visual = visual.withColumn(\"d_pedido_2\", visual.d_pedido.cast(StringType())).drop(\"d_pedido\").withColumnRenamed(\"d_pedido_2\", \"d_pedido\")\n",
    "#\n",
    "visual = visual.join(result, visual.codigo_cliente == result.codigo_cliente).drop(result[\"codigo_cliente\"])\n",
    "visual = visual.select(['codigo_cliente','count(codigo_pedido)','idade','pedidos'])\n",
    "visual = visual.dropDuplicates()\n",
    "visual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual.select('pedidos').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(visual.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual.select('pedidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import udf, col\n",
    "\n",
    "#join_udf = udf(lambda x: \"[\" + \",\".join(str(x)) + \"]\", StringType())\n",
    "#visual.withColumn(\"pedidos_str\", join_udf(visual.pedidos)).show()\n",
    "\n",
    "#visual = visual.drop(visual['pedidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual.write.json('pedidos.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
